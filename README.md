# PREPARE-Challenge-Phase-3
Proof of Principle Demonstration for Phase 3 of PREPARE CHALLENGE  "Pioneering Research for Early Prediction of Alzheimer's and Related Dementias EUREKA Challenge" .

Link to the phase 3 of the competition: https://www.drivendata.org/competitions/304/prepare-challenge-phase-3/

The project structure is based on https://cookiecutter-data-science.drivendata.org/.

Author: Gianpaolo Tomasi

Team: GiaPaoDawei

Licence: MIT


## Summary
The challenge is centered around developing better methods for prediction of Alzheimer's disease and Alzheimer's disease related dementias (AD/ADRD) focuing in particular on early diagnosis. 
Phase 3 is focused on the imporvement of the winning models of Phase 2 for both social determinant and acoustic tracks.


## Setup

1) clone this repo
   
git clone https://github.com/giampiportatile/PREPARE-Challenge-Phase-3
   
2) create the virtual environment

conda create -n PREPARE_phase3 python=3.10

3) Install the required packages from the requirements.txt file shared in this directory:

pip install -r requirements.txt


## Data
Copy the 5 files of the competition (social determinants track) into the data/raw folder.

Note that these files are not available anymore on the DrivenData website, so we will assume the user downloaded them during the competition.

train_features.csv

train_labels.csv

test_features.csv

submission_format.csv

test_labels.csv (this file is used only to compute the score on the test data)


## Parse data and train model 

1) data preprocessing and feature engineering
   
python src\preprocess_data.py

2) train models

python src\train_and_predict.py

#### Please execute the previous scripts from the 'base directory' which is the directory where the github repo has been cloned.

## Post-prediction analysis: confidence intervals, Shapley Values and visualization
1) To compute and display confidence intervals with different conformal prediction methods, please execute

python src\mapie_confidence_intervals.py

2) To compute and display Shapley values, prediction and 50% and 90% confidence intervals for a given subject, please execute

python src\shap_and_visualisation.py


#### Please execute the previous scripts from the 'base directory' which is the directory where the github repo has been cloned.

## Models
All the model trained by train_and_predict.py are already saved in the 'model' folder in case the user wants to employ them directly (with the exception of the TabPFN model which is more than 100 Mb and could not be saved in github).

## Predictions
All the predictions generated by train_and_predict.py are already saved in the 'predictions' folder in case the user wants to test them directly. 

For TabM we saved 5 different iterations which differ for the seed used to train the network and we also saved the forecasts for up to 30 epochs
in order to allow the user to analyse the impact of random initialisation and epochs on the performance.

## Jupyter notebook

Please execute

jupyter notebooks\example.ipynb


## Other: Application of Shapley values for recursive feature elimination on the MHAS data

Downlad the harmonized MHAS data from https://www.mhasweb.org/DataProducts/HarmonizedData.aspx
Save the 'H_MHAS_c2.dta files into the analysis_of_MHAS_data folder

1) To pre-process the MHAS dataset 

python analysis_of_MHAS_data\load_and_preprocess_data.py


2) As second step, in order to perform the recursive feature elimination using Shapley Values, execute

python analysis_of_MHAS_data\shapley_based_RFE.py
   




